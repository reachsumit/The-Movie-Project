---
title: "Revenue Prediction model"
author: "Garima Sood"
date: "March 12, 2018"
output: html_document
---

```{r}
install.packages("gridExtra")
install.packages("dplyr")
install.packages("lubridate")
install.packages("Metrics")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("randomForest")
library(gridExtra)
library(dplyr)
library(lubridate)
library(Metrics)
library(rpart)
library(rpart.plot)
library(randomForest)


dataPath <- "C:/Users/garim/Documents/Quarter 2/Data Mining/Project/Merged data"
master_data <- read.csv(paste(dataPath,"master_data_with_imputed_budget_and_revenue.csv", sep = "/"))
summary(master_data)
```

**Data Processsing**

```{r}
master_data$release_date <- as.Date(master_data$release_date)

#To cut the impact of inflation on movie revenues & budgets, I am excluding data of movies released before Jan 1985

master_data <- master_data[master_data$release_date > as.Date("01/01/1985","%m/%d/%Y"),]
master_data <- master_data[master_data$budget > 0,]
master_data$actor_1_gender <- as.factor(ifelse(master_data$actor_1_gender==0,NA,ifelse(master_data$actor_1_gender==2,"Male","Female")))
master_data$actor_2_gender <- as.factor(ifelse(master_data$actor_2_gender==0,NA,ifelse(master_data$actor_2_gender==2,"Male","Female")))
master_data$actor_3_gender <- as.factor(ifelse(master_data$actor_3_gender==0,NA,ifelse(master_data$actor_3_gender==2,"Male","Female")))
master_data$actor_4_gender <- as.factor(ifelse(master_data$actor_4_gender==0,NA,ifelse(master_data$actor_4_gender==2,"Male","Female")))
master_data$actor_5_gender <- as.factor(ifelse(master_data$actor_5_gender==0,NA,ifelse(master_data$actor_5_gender==2,"Male","Female")))
master_data$director_gender <- as.factor(ifelse(master_data$director_gender==0,NA,ifelse(master_data$director_gender==2,"Male","Female")))
master_data$producer_gender <- as.factor(ifelse(master_data$producer_gender==0,NA,ifelse(master_data$producer_gender==2,"Male","Female")))
master_data$collection <- as.factor(ifelse(nchar(as.character(master_data$belongs_to_collection))>0,"Yes","No"))

master_data$num_prod_comp <-(master_data$production_company_1!="")+(master_data$production_company_2!="")+
                            (master_data$production_company_3!="")

master_data$num_prod_ctry <-(master_data$production_country_1!="")+(master_data$production_country_2!="")+
                            (master_data$production_country_3!="")

master_data$release_month <- month.abb[month(master_data$release_date)]

master_data <- master_data[ , -which(names(master_data) %in% 
              c( "movie_id" ,"actor_1_name","actor_2_name","actor_3_name","actor_4_name","actor_5_name","director_name","producer_name",
                 "casting_gender","casting_name","belongs_to_collection","genre_2","genre_3","genre_4","production_company_1",
                 "production_company_2","production_company_3" ,"production_country_1", "production_country_2",  "production_country_3" , "spoken_language_1","spoken_language_2", "spoken_language_3" ,"homepage","imdb_id" ,"original_title","overview","poster_path", "status","title","video"))]

require(ggplot2)

p1<- ggplot(master_data, aes(x = actor_1_gender)) + geom_bar()
p2<- ggplot(master_data, aes(x = actor_2_gender)) + geom_bar()
p3<- ggplot(master_data, aes(x = actor_3_gender)) + geom_bar()
p4<- ggplot(master_data, aes(x = actor_4_gender)) + geom_bar()
p5<- ggplot(master_data, aes(x = actor_5_gender)) + geom_bar()
p6<- ggplot(master_data, aes(x = director_gender)) + geom_bar()
p7<- ggplot(master_data, aes(x = producer_gender)) + geom_bar()
p8<- ggplot(master_data, aes(x = budget)) + geom_histogram()
p9<- ggplot(master_data, aes(x = revenue)) + geom_histogram()
p10<-ggplot(master_data, aes(x = popularity)) + geom_histogram()
p11<-ggplot(master_data, aes(x = vote_average)) + geom_histogram()
p12<-ggplot(master_data, aes(x = vote_count)) + geom_histogram()


grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12, nrow = 4, ncol=3)
```

Plots show that there are a lot of NA values in the different columns. Counting the NA values per column in the data

```{r}
perc_na <- function(x){
  return(sum(is.na(x))/length(x))
}

round(apply(master_data, 2, function(x) perc_na(x)),2)

master_data$na_count <- apply(master_data, 1, function(x) sum(is.na(x)))
table(master_data$na_count)
```

Deleting records with missing data in more than 9 columns, and checking the poportion of missing values in the updated data set

```{r}
data <- master_data[master_data$na_count<9,]
dim(data)

round(apply(data, 2, function(x) perc_na(x)),2)
```

After removing thee records, we are left with about 10% missing values in the gender column of lead actors, and 24% missing values in budget. Rest of the columns look good. 

Checking the distribution of our dependent variable (revenue)

```{r}

data$quartile <- ntile(data$revenue, 5)
size = as.matrix(table(data$quartile))
cbind(aggregate(data$revenue, by = list(data$quartile), mean), size)
ggplot(data, aes(x = revenue)) + geom_histogram()

```


Missing value estimation:
```{r}
mean_impute <- function(x){
  a<- (mean(x[!is.na(x)]))
  x <- ifelse(is.na(x), a, x)
  return(x)
}

median_impute <-  function(x){
  a<- (median(x[!is.na(x)]))
  x <- ifelse(is.na(x), a, x)
  return(x)
}

mode_impute <-  function(x){
  ux <- (unique(x))
  a<-ux[which.max(tabulate(match(x[!is.na(x)], ux)))]
  x <- ifelse(is.na(x), a, x)
  return(x)
}

data.imp <-data

#Imputing missing data in gender and runtime columns using mode and median respectively

data.imp$actor_1_gender <- as.factor(mode_impute(data$actor_1_gender))
data.imp$actor_2_gender <- as.factor(mode_impute(data$actor_2_gender))
data.imp$runtime <- median_impute(data.imp$runtime)

perc_blank <- function(x){
  return(sum(x ==""|x==" ")/length(x))
}
round(apply(data.imp, 2, function(x) perc_blank(x)),2)

```

Removing the extreme budget records
```{r}
sd.budget <- sqrt(var(data.imp$budget))
sd.budget*6
length(data.imp$budget[data.imp$budget>2e+08])
data.imp <- data.imp[data.imp$budget<2e+08,]
```

```{r}
data.final <-data.imp[,-which(names(data.imp) %in% c( "actor_2_gender", "actor_3_gender", "actor_4_gender", "actor_5_gender",  "director_gender", "producer_gender", 'na_count', 'genre_1', 'adult','tagline','original_language'))]
data.final$actor_1_gender <- as.factor(ifelse(data.final$actor_1_gender==2, "Male", "Female"))

#Splitting the data into test & train
data.final$budget <- scale(data.final$budget)

c <- round(nrow(data.final)*0.7,0)
s <- sample(1:nrow(data.final), c)

train <- data.final[s,]
test <- data.final[-s,]
```

Building a model to predict revenue of the movie before it is released. I will not model the vote count and vote average variables as they are collected after the release of the movie.

Build a multiple linear model for revenue prediction

```{r}

ggplot(data.final,aes(budget,revenue,colour = collection)) + geom_point()
ggplot(data.final,aes(budget,revenue,colour = actor_1_gender)) + geom_point()
ggplot(data.final,aes(popularity,revenue)) + geom_point()
ggplot(data.final,aes(runtime,revenue)) + geom_point(color = "royal blue")
ggplot(data.final,aes(vote_average,revenue)) + geom_point()
ggplot(data.final,aes(vote_count,revenue)) + geom_point()
ggplot(data.final,aes(num_prod_comp,revenue)) + geom_bar(stat = "identity", color = "sky blue")
ggplot(data.final,aes(num_prod_ctry ,revenue)) + geom_bar(stat = "identity", color = "sky blue")
ggplot(data=data.final, aes(x=release_month, y=revenue)) + geom_bar(stat = 'identity', color = 'sky blue')

```

Cleary vote count has an increasing relationship with revenue as that indicates # people who went to watch the movie. But I cannot model vote average and vote count for predicting revenue as they are collected after the movie is released

**Fitting a linear model**
```{r}

revenue_pred <- lm(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+ budget, data = train)
summary(revenue_pred)

rmseTest <- rmse((predict(revenue_pred, test)),test$revenue)
rmseTrain <-rmse((predict(revenue_pred, train)),train$revenue)

#Validation
cbind(rmseTrain, rmseTest)
```

The linear model does not give a good fit and the resons could be:

+ interaction between variables e.g.: popularity is driven by actors, budget is driven by runtime- number of countries in which the movie is launched

+ Scatter plots do not show a perfect linear relationship between the dependent and independent variables

**Fitting a regression tree model**
to take into account the interactions between variables
```{r}
revenue_tree <- rpart(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+num_prod_ctry + budget , data = train)

plotcp(revenue_tree)
x <- printcp(revenue_tree)

rsq.rpart(revenue_tree)

# Retreive optimal cp value based on cross-validated error
opt_index <- which.min(revenue_tree$cptable[, "xerror"])
cp_opt <- revenue_tree$cptable[opt_index, "CP"]

revenue_tree_opt <- prune(tree = revenue_tree, 
                         cp = cp_opt)

# Display the pruned tree results
rpart.plot(x = revenue_tree_opt, yesno = 2, type = 1, extra = 1)

rsq.rpart(revenue_tree_opt)
```

Looking at the accuracy and confusion matrix from tree model (test vs. train)
```{r}

predictedTest <- predict(revenue_tree_opt, test)
rmseTest <- rmse((test$revenue),predictedTest)
rmseTrain <- rmse((train$revenue),predict(revenue_tree_opt, train))

r_sqTest <- 1-rmseTest^2/var(test$revenue)
r_sqTr <- 1-rmseTrain^2/var(train$revenue)


a<- round(rbind(cbind(rmseTrain, rmseTest), cbind(r_sqTr, r_sqTest)),2)
rownames(a)<- c("RMSE", "R-sq")
colnames(a)<- c("Train", "Test")
a

```

The fit of regression tree is similar to the linear regression model. I will fit random forests to see if the accuracy can be improved further by bootstrapping: 

```{r}
rfor1 <- randomForest(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+num_prod_ctry + budget, data= train)
```

```{r}
rfor1
rmseTest <- rmse(predict(rfor1, test), test$revenue)
rmseTrain <- rmse(predict(rfor1, train), train$revenue)
Rsq_test <- 1-(rmseTest)^2/var(test$revenue)
R_sqTr <- 1-(rmseTrain)^2/var(train$revenue)

a<- round(rbind(cbind(rmseTrain, rmseTest), cbind(R_sqTr, Rsq_test)),2)
rownames(a)<- c("RMSE", "R-sq")
colnames(a)<- c("Train", "Test")
a

```

I see overfitting in this model. Let me set the node size to avoid this:
```{r}
for (i in c(20,25,30,35,40)){

  rfor2 <- randomForest(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+ budget, data= train, nodesize = i)
  rmseTest <- rmse(predict(rfor2, test), test$revenue)
  rmseTrain <- rmse(predict(rfor2, train), train$revenue)
  Rsq_test <- 1-(rmseTest)^2/var(test$revenue)
  R_sqTr <- 1-(rmseTrain)^2/var(train$revenue)
  
  a<- round(rbind(cbind(rmseTrain, rmseTest), cbind(R_sqTr, Rsq_test)),2)
  rownames(a)<- c("RMSE", "R-sq")
  colnames(a)<- c("Train", "Test")
  print(i)
  print(a)
}
```

I will go with the node size 25. The model fit has clearly improved with random forests compared to the tree model

```{r}
rfor.f <- randomForest(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+ budget, data= train, nodesize = 30)
rfor.f$importance
```

**Performing cluster wise class regression**
```{r}

#clustreg function
clustreg=function(dat,k,tries,sed,niter){
  
  set.seed(sed)
  dat=as.data.frame(dat)
  rsq=rep(NA,niter)
  res=list()
  rsq.best=0
  for(l in 1:tries) {
    
    c = sample(1:k,nrow(dat),replace=TRUE)
    yhat=rep(NA,nrow(dat))
    for(i in 1:niter) {		
      resid=pred=matrix(0,nrow(dat),k)
      for(j in 1:k){	
        pred[,j]=predict(glm(dat[c==j,],family="gaussian"),newdata=dat)		
        resid[,j] = (pred[,j]-dat[,1])^2
      }
      
      c = apply(resid,1,which.min)
      for(m in 1:nrow(dat)) {yhat[m]=pred[m,c[m]]}
      rsq[i] = cor(dat[,1],yhat)^2	
      #print(rsq[i])
    }
    
    if(rsq[niter] > rsq.best) {	
      rsq.best=rsq[niter]
      l.best=l
      c.best=c
      yhat.best=yhat
    }
  }
  
  for(i in k:1) res[[i]]=summary(lm(dat[c.best==i,]))
  
  return(list(data=dat,nclust=k,tries=tries,seed=sed,rsq.best=rsq.best,number.loops=niter, Best.try=l.best,cluster=c.best,results=res))
}

clustreg.predict=function(results,newdat){
  
  yhat=rep(NA,nrow(newdat))
  resid=pred=matrix(0,nrow(newdat),length(table(results$cluster)))
  
  for(j in 1:length(table(results$cluster))){			
    pred[,j]=predict(glm(results$data[results$cluster==j,],family="gaussian"),newdata=newdat)		
    resid[,j] = (pred[,j]-newdat[,1])^2
  }
  
  c = apply(resid,1,which.min)
  for(m in 1:nrow(newdat)) {yhat[m]=pred[m,c[m]]}
  rsq = cor(newdat[,1],yhat)^2	
  
  return(list(results=results,newdata=newdat,cluster=c,yhat=yhat,rsq=rsq))
  
}
```

Trying multiple clusters and assessing fits using approximated R-sq and RMSE
```{r}

for (i in 2:5){
  
  rev_clust<- clustreg(train[,c(5,1,2,3,6,9,10,11)],i,100,881,50)
  ypredTr<- clustreg.predict(rev_clust, train[,c(5,1,2,3,6,9,10,11)])
  ypredTest <- clustreg.predict(rev_clust,test[,c(5,1,2,3,6,9,10,11)])
  yhat_tr<- ypredTr$yhat
  yhat_test<- ypredTest$yhat
  
  rmseTest <- rmse(yhat_test, test$revenue)
  rmseTrain <- rmse(yhat_tr, train$revenue)
  Rsq_test <- 1-(rmseTest)^2/var(test$revenue)
  R_sqTr <- 1-(rmseTrain)^2/var(train$revenue)
  
  a<- round(rbind(cbind(rmseTrain, rmseTest), cbind(R_sqTr, Rsq_test)),2)
  rownames(a)<- c("RMSE", "R-sq")
  colnames(a)<- c("Train", "Test")
  print(i)
  x <- table(rev_clust$cluster)
  print(x)
  print(a)
  
}

```

Cluster wise regression is fitting really well. Based on the size of the clusters, I will choose the 3 cluster solution as clusters get thin beyond that.

```{r}
rev_clust3<- clustreg(train[,c(5,1,2,3,6,9,10,11)],3,100,881,50)
rev_clust3$result
```
 
```{r}
train_clus <- as.data.frame(cbind(train, cluster = rev_clust3$cluster))
train_clus$cluster <- as.factor(train_clus$cluster)
cbind(aggregate(train_clus[,c(2,5,3,6,7,8,10)], by = list(train_clus$cluster), mean), size = table(train_clus$cluster))
ggplot(train_clus,aes(budget,revenue,colour = cluster)) + geom_point()
```

Fitting a classification tree on train data to classify new datasets into clusters for revenue predictions through clusterwise regression results 
```{r}
s <- sample(1:nrow(train), round(0.7*nrow(train),0))
train1 <- train_clus[s,]
train2 <- train_clus[-s,]

  classify <- randomForest(cluster~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+ budget, data= train1, nodesize = 10)
  train1clus <- predict(classify, train1, type = "class")
  train2clus <- predict(classify, train2, type = "class")
  round(prop.table(table(actual = train1$cluster, pred = train1clus),1),2)
  round(prop.table(table(actual = train2$cluster, pred = train2clus),1),2)
  
  accTrain1 <- sum(train1clus==train1$cluster)/nrow(train1); accTrain1
  accTrain2 <- sum(train2clus==train2$cluster)/nrow(train2); accTrain2

```
  
The accuracy of classification model for deciding clusters is low for small sized clusters. I would weigh in this factor to decide between random forest and cluster wise regression result. The accuracy of predicting revenue once the clusters are identified is very high but when combined with process of identifying the right clusters, expected accuracy drop close to that of random forests.

